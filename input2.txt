Entropy is a mathematically-defined thermodynamic quantity that helps to account for the flow of energy through a thermodynamic process. Entropy was originally defined for a thermodynamically reversible process as

where the uniform temperature (T) of a closed system is divided into an incremental reversible transfer of heat energy into that system (dQ). The above definition is sometimes called the macroscopic definition of entropy because it can be used without regard to any microscopic picture of the contents of a system. In thermodynamics, entropy has been found to be more generally useful and it has several other reformulations. Entropy was discovered when it was noticed via mathematics to be a quantity that behaves as a function of state. Entropy is an extensive property, but it is often given as an intensive property of specific entropy as entropy per unit mass or entropy per mole.
In statistical mechanics, entropy is often related to the notions of order and disorder . In the modern microscopic interpretation of entropy in statistical mechanics entropy is defined as the amount of additional information needed to specify the exact physical state of a system, given its thermodynamic specification. Various thermodynamic processes thus can be reduced to a description of how that information changes as the system evolves from its initial condition. It is often said that entropy is an expression of disorder or randomness, although those concepts lack clear definitions except in terms of entropy itself. The second law is now often seen as an expression of the fundamental postulate of statistical mechanics via the modern definition of entropy.

The analysis which led to the concept of entropy began with the work of French mathematician Lazare Carnot who in his 1803 paper Fundamental Principles of Equilibrium and Movement proposed that in any machine the accelerations and shocks of the moving parts represent losses of moment of activity. In other words, in any natural process there exists an inherent tendency towards the dissipation of useful energy. Building on this work, in 1824 Lazare's son Sadi Carnot published Reflections on the Motive Power of Fire which posited that in all heat-engines whenever "caloric", or what is now known as heat, falls through a temperature difference, work or motive power can be produced from the actions of the "fall of caloric" between a hot and cold body. He made the analogy with that of how water falls in a water wheel. This was an early insight into the second law of thermodynamics.[1] Carnot based his views of heat partially on the early 18th century "Newtonian hypothesis" that both heat and light were types of indestructible forms of matter, which are attracted and repelled by other matter, and partially on the contemporary views of Count Rumford who showed (1789) that heat could be created by friction as when cannon bores are machined.[2] Carnot reasoned that if the body of the working substance, such as a body of steam, is returned to its original state at the end of a complete engine cycle, that "no change occurs in the condition of the working body".
The first law of thermodynamics, formalized based on the heat-friction experiments of James Joule in 1843, deals with the concept of energy, which is conserved in all processes; the first law, however, is unable to quantify the effects of friction and dissipation.
In the 1850s and 1860s, German physicist Rudolf Clausius objected to the supposition that no change occurs in the working body, and gave this "change" a mathematical interpretation by questioning the nature of the inherent loss of usable heat when work is done, e.g. heat produced by friction.[3] Clausius described entropy as the transformation-content, i.e. dissipative energy use, of a thermodynamic system or working body of chemical species during a change of state.[3] This was in contrast to earlier views, based on the theories of Isaac Newton, that heat was an indestructible particle that had mass.
Later, scientists such as Ludwig Boltzmann, Josiah Willard Gibbs, and James Clerk Maxwell gave entropy a statistical basis. In 1877 Boltzmann visualized a probabilistic way to measure the entropy of an ensemble of ideal gas particles, in which he defined entropy to be proportional to the logarithm of the number of microstates such a gas could occupy. Henceforth, the essential problem in statistical thermodynamics, i.e. according to Erwin Schrödinger, has been to determine the distribution of a given amount of energy E over N identical systems. Carathéodory linked entropy with a mathematical definition of irreversibility, in terms of trajectories and integrability.

There are two related definitions of entropy: the thermodynamic definition and the statistical mechanics definition. Historically, the classical thermodynamics definition developed first, and it has more recently been extended in the area of non-equilibrium thermodynamics. Entropy was defined from a classical thermodynamics viewpoint, in which the detailed quantum constituents are not directly considered, with their behavior only showing up in macroscopically averaged properties, e.g. heat capacity.. Later, thermodynamic entropy was more generally defined from a statistical thermodynamics viewpoint, in which the detailed quantum constituents (photons, phonons, spins, etc.) are explicitly considered.

Function of state
There are many thermodynamic properties that are functions of state. This means that at a particular state, these properties have a certain value. Often, if two properties have a particular value, then the state is determined and the other properties values are set. For instance, an ideal gas, at a particular temperature and pressure, has a particular volume according to the ideal gas equation. That entropy is a function of state is what makes it very useful. As another instance, a pure substance of single phase at a particular uniform temperature and pressure (and thus a particular state) is at not only a particular volume but also at a particular entropy.[5] In the Carnot cycle, the working fluid returns to the same state at particular part of the cycle, hence the loop integral equaling zero.

Reversible process
Entropy is defined for a reversible process and for a system that, at all times, can be treated as being at a uniform state and thus at a uniform temperature. Reversibility is an ideal that some real processes approximate and that is often presented in study exercises. For a reversible process, entropy behaves as a conserved quantity and no change occurs in total entropy. More specifically, total entropy is conserved in a reversible process and not conserved in a irreversible process.[6] One has to be careful about system boundaries. For example, in the Carnot cycle, while the heat flow from the hot reservoir to the cold reservoir represents an increase in entropy, the work output, if reversibly and perfectly stored in some energy storage mechanism, represents a decrease in entropy that could be used to operate the heat engine in reverse and return to the previous state, thus the total entropy change is still zero at all times if the entire process is reversible. Any process that does not meet the requirements of a reversible process must be treated as an irreversible process, which is usually a complex task. An irreversible process increases entropy.

Heat transfer situations require two or more non-isolated systems in thermal contact. In irreversible heat transfer, heat energy is irreversibly transferred from the higher temperature system to the lower temperature system, and the combined entropy of the systems increases. Each system, by definition, must have its own absolute temperature applicable within all areas in each respective system in order to calculate the entropy transfer. Thus, when a system at higher temperature TH transfers heat dQ to a system of lower temperature TC, the former loses entropy dQ/TH and the latter gains entropy dQ/TC. The combined entropy change is dQ/TC-dQ/TH which is positive, reflecting an increase in the combined entropy. When calculating entropy, the same requirement of having an absolute temperature for each system in thermal contact exchanging heat also applies to the entropy change of an isolated system having no thermal contact.



